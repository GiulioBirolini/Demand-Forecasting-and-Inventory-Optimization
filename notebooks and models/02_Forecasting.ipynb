{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782222ea-0463-4668-8e1d-f6842e6fcfd3",
   "metadata": {},
   "source": [
    "##### This notebook is dedicated to preparing the data and training different regression models to forecast weekly sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c95257-e467-4ebf-9899-a19872bc7e1c",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the cleaned dataset created in the previous notebook (`train_merged_clean.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505ec3db-9d6f-40ab-be69-7beb70cd4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/train_merged_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de6425-900d-4be7-a5c7-bd033bb4758b",
   "metadata": {},
   "source": [
    "### Check Features\n",
    "- Inspect numeric and categorical columns\n",
    "- Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea94eebc-799b-4f0e-80de-56320b5f368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420285 entries, 0 to 420284\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         420285 non-null  int64  \n",
      " 1   Dept          420285 non-null  int64  \n",
      " 2   Date          420285 non-null  object \n",
      " 3   Year          420285 non-null  int64  \n",
      " 4   Month         420285 non-null  int64  \n",
      " 5   Quarter       420285 non-null  int64  \n",
      " 6   ISO_Week      420285 non-null  int64  \n",
      " 7   Weekly_Sales  420285 non-null  float64\n",
      " 8   IsHoliday     420285 non-null  bool   \n",
      " 9   Type          420285 non-null  object \n",
      " 10  Size          420285 non-null  int64  \n",
      " 11  Temperature   420285 non-null  float64\n",
      " 12  Fuel_Price    420285 non-null  float64\n",
      " 13  CPI           420285 non-null  float64\n",
      " 14  Unemployment  420285 non-null  float64\n",
      "dtypes: bool(1), float64(5), int64(7), object(2)\n",
      "memory usage: 45.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c29cd-5bbc-41db-9975-348ed1e1dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store           0\n",
       "Dept            0\n",
       "Date            0\n",
       "Year            0\n",
       "Month           0\n",
       "Quarter         0\n",
       "ISO_Week        0\n",
       "Weekly_Sales    0\n",
       "IsHoliday       0\n",
       "Type            0\n",
       "Size            0\n",
       "Temperature     0\n",
       "Fuel_Price      0\n",
       "CPI             0\n",
       "Unemployment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e4f355-5bba-4bc6-9998-b143da34b5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>ISO_Week</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "      <td>420285.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.195477</td>\n",
       "      <td>44.242771</td>\n",
       "      <td>2010.968443</td>\n",
       "      <td>6.449709</td>\n",
       "      <td>2.482920</td>\n",
       "      <td>25.827729</td>\n",
       "      <td>16030.329773</td>\n",
       "      <td>136749.569176</td>\n",
       "      <td>60.090474</td>\n",
       "      <td>3.360888</td>\n",
       "      <td>171.212152</td>\n",
       "      <td>7.960077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.787213</td>\n",
       "      <td>30.507197</td>\n",
       "      <td>0.796893</td>\n",
       "      <td>3.243394</td>\n",
       "      <td>1.071464</td>\n",
       "      <td>14.152442</td>\n",
       "      <td>22728.500149</td>\n",
       "      <td>60992.688568</td>\n",
       "      <td>18.448260</td>\n",
       "      <td>0.458523</td>\n",
       "      <td>39.162280</td>\n",
       "      <td>1.863873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34875.000000</td>\n",
       "      <td>-2.060000</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>126.064000</td>\n",
       "      <td>3.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2117.560000</td>\n",
       "      <td>93638.000000</td>\n",
       "      <td>46.680000</td>\n",
       "      <td>2.933000</td>\n",
       "      <td>132.022667</td>\n",
       "      <td>6.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7659.090000</td>\n",
       "      <td>140167.000000</td>\n",
       "      <td>62.090000</td>\n",
       "      <td>3.452000</td>\n",
       "      <td>182.350989</td>\n",
       "      <td>7.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>20268.380000</td>\n",
       "      <td>202505.000000</td>\n",
       "      <td>74.280000</td>\n",
       "      <td>3.738000</td>\n",
       "      <td>212.445487</td>\n",
       "      <td>8.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>693099.360000</td>\n",
       "      <td>219622.000000</td>\n",
       "      <td>100.140000</td>\n",
       "      <td>4.468000</td>\n",
       "      <td>227.232807</td>\n",
       "      <td>14.313000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Store           Dept           Year          Month  \\\n",
       "count  420285.000000  420285.000000  420285.000000  420285.000000   \n",
       "mean       22.195477      44.242771    2010.968443       6.449709   \n",
       "std        12.787213      30.507197       0.796893       3.243394   \n",
       "min         1.000000       1.000000    2010.000000       1.000000   \n",
       "25%        11.000000      18.000000    2010.000000       4.000000   \n",
       "50%        22.000000      37.000000    2011.000000       6.000000   \n",
       "75%        33.000000      74.000000    2012.000000       9.000000   \n",
       "max        45.000000      99.000000    2012.000000      12.000000   \n",
       "\n",
       "             Quarter       ISO_Week   Weekly_Sales           Size  \\\n",
       "count  420285.000000  420285.000000  420285.000000  420285.000000   \n",
       "mean        2.482920      25.827729   16030.329773  136749.569176   \n",
       "std         1.071464      14.152442   22728.500149   60992.688568   \n",
       "min         1.000000       1.000000       0.000000   34875.000000   \n",
       "25%         2.000000      14.000000    2117.560000   93638.000000   \n",
       "50%         2.000000      26.000000    7659.090000  140167.000000   \n",
       "75%         3.000000      38.000000   20268.380000  202505.000000   \n",
       "max         4.000000      52.000000  693099.360000  219622.000000   \n",
       "\n",
       "         Temperature     Fuel_Price            CPI   Unemployment  \n",
       "count  420285.000000  420285.000000  420285.000000  420285.000000  \n",
       "mean       60.090474       3.360888     171.212152       7.960077  \n",
       "std        18.448260       0.458523      39.162280       1.863873  \n",
       "min        -2.060000       2.472000     126.064000       3.879000  \n",
       "25%        46.680000       2.933000     132.022667       6.891000  \n",
       "50%        62.090000       3.452000     182.350989       7.866000  \n",
       "75%        74.280000       3.738000     212.445487       8.567000  \n",
       "max       100.140000       4.468000     227.232807      14.313000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f45c92-a922-4702-97ae-1b415658d0e1",
   "metadata": {},
   "source": [
    "### One-Hot Encoding and Drop Irrelevant Column\n",
    "- Transform categorical variable `Type` using one-hot encoding.  \n",
    "- `drop_first=True` avoids collinearity for linear models.\n",
    "- Drop `Date` because I already have `Year`, `Month`, `Quarter`, `ISO_Week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666cccac-aecd-44a9-b72e-f786747b87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb5f7da-2692-4f6e-b7a0-302e7cdcfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=[\"Type\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b18460d-7468-44b8-be70-1de3475d1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  Dept        Date  Year  Month  Quarter  ISO_Week  Weekly_Sales  \\\n",
      "0      1     1  2010-02-05  2010      2        1         5      24924.50   \n",
      "1      1     1  2010-02-12  2010      2        1         6      46039.49   \n",
      "2      1     1  2010-02-19  2010      2        1         7      41595.55   \n",
      "3      1     1  2010-02-26  2010      2        1         8      19403.54   \n",
      "4      1     1  2010-03-05  2010      3        1         9      21827.90   \n",
      "\n",
      "   IsHoliday    Size  Temperature  Fuel_Price         CPI  Unemployment  \\\n",
      "0      False  151315        42.31       2.572  211.096358         8.106   \n",
      "1       True  151315        38.51       2.548  211.242170         8.106   \n",
      "2      False  151315        39.93       2.514  211.289143         8.106   \n",
      "3      False  151315        46.63       2.561  211.319643         8.106   \n",
      "4      False  151315        46.50       2.625  211.350143         8.106   \n",
      "\n",
      "   Type_B  Type_C  \n",
      "0   False   False  \n",
      "1   False   False  \n",
      "2   False   False  \n",
      "3   False   False  \n",
      "4   False   False  \n"
     ]
    }
   ],
   "source": [
    "print(df_dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ce7a6c-de24-4119-9014-af1742df28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dummies.drop(columns=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff9cce-d41e-45c7-9a86-dc0fb544e8bc",
   "metadata": {},
   "source": [
    "### Split Train / Validation / Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed369177-8378-4321-a954-4058e0dabed5",
   "metadata": {},
   "source": [
    "- I create a small **final test set** (10%) to evaluate the model after tuning.\n",
    "- I use the remaining data to create **training (80%)** and **validation (20%)** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987defea-1317-4eec-b07b-e092106df302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (285793, 14)\n",
      "Validation shape: (71449, 14)\n",
      "Test shape: (63043, 14)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Weekly_Sales'])\n",
    "y = df['Weekly_Sales']\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898b2b-5ffa-47b4-ab82-c5f92c8f2631",
   "metadata": {},
   "source": [
    "### Scale the features and Train Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c92a2d-4d0e-4229-aada-d6eb483c1435",
   "metadata": {},
   "source": [
    "Linear models and SGD need scaling, I use `StandardScaler`\n",
    "- Linear models: LinearRegression, Ridge, Lasso, ElasticNet.\n",
    "- SGDRegressor, more suitable for large datasets.\n",
    "- Ensemble methods: RandomForestRegressor, GradientBoostingRegressor.\n",
    "- I compare **RMSE on validation set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85d84473-217e-4dde-b8e2-86e5b3b739ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression RMSE on validation set: 21679.08\n",
      "Ridge RMSE on validation set: 21679.08\n",
      "Lasso RMSE on validation set: 21679.02\n",
      "ElasticNet RMSE on validation set: 21817.93\n",
      "SGDRegressor RMSE on validation set: 21698.71\n",
      "RandomForest RMSE on validation set: 3771.29\n",
      "GradientBoosting RMSE on validation set: 10681.57\n"
     ]
    }
   ],
   "source": [
    "linear_models = {\n",
    "    \"LinearRegression\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", LinearRegression())]),\n",
    "    \"Ridge\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", Ridge(max_iter=10000))]),\n",
    "    \"Lasso\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", Lasso(max_iter=10000))]),\n",
    "    \"ElasticNet\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", ElasticNet(max_iter=10000))]),\n",
    "    \"SGDRegressor\": Pipeline([(\"scaler\", StandardScaler()), (\"model\", SGDRegressor(max_iter=5000, tol=1e-4, random_state=42))])\n",
    "}\n",
    "tree_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "models = {**linear_models, **tree_models}    \n",
    "    \n",
    "rmse_results={}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    rmse_results[name] = rmse\n",
    "    print(f\"{name} RMSE on validation set: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312aad3-efc0-453f-a20b-4cecd6325713",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "- **Linear Models** (LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor)  \n",
    "  - All give similar RMSE (~21,600).  \n",
    "  - Linear relationships only; regularization has little effect with default parameters.  \n",
    "\n",
    "- **Tree-Based Models** (RandomForest, GradientBoosting)  \n",
    "  - Perform much better (RandomForest ~3,771, GradientBoosting ~9,688).  \n",
    "  - Capture non-linear patterns and feature interactions naturally.  \n",
    "\n",
    "- **Reason for Gap**  \n",
    "  - The target likely depends on non-linear relationships.  \n",
    "  - Linear models cannot model these without feature engineering.  \n",
    "  - Tree ensembles handle complexity directly, leading to lower RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c66975-ef49-4f26-8484-9d614bf88328",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd6319-8b72-413c-9744-99d5afa12ad1",
   "metadata": {},
   "source": [
    "In this step, I focus on improving **Random Forest** and **Gradient Boosting** models by tuning their hyperparameters. \n",
    "\n",
    "I use `HalvingRandomSearchCV` with cross-validation to efficiently explore different combinations of parameters:\n",
    "\n",
    "- **Random Forest:** `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`.\n",
    "- **Gradient Boosting:** `n_estimators`, `learning_rate`, `max_depth`, `min_samples_split`, `min_samples_leaf`.\n",
    "\n",
    "HalvingRandomSearchCV speeds up tuning by discarding poor hyperparameter candidates early and allocating more resources only to the promising ones. It is more efficient than `RandomizedSearchCV`, especially on big data and complex models.\n",
    "\n",
    "These two tree-based models are selected because, in our earlier evaluation, they significantly outperformed linear models in terms of RMSE. Focusing on them allows me to maximize predictive performance while keeping the process efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3001b907-57b7-428a-bb73-14c2847f7aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 20000\n",
      "max_resources_: 285793\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 14\n",
      "n_resources: 20000\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 5\n",
      "n_resources: 60000\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 180000\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Total elapsed time: 4064.75 seconds\n",
      "Random Forest Best Params: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None}\n",
      "Random Forest Validation RMSE: 3831.9476672197047\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_search = HalvingRandomSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_param_grid,\n",
    "    factor=3,  \n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5, \n",
    "    resource='n_samples',\n",
    "    min_resources=20000,\n",
    "    max_resources='auto',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "rf_preds = rf_best.predict(X_val)\n",
    "print(\"Random Forest Best Params:\", rf_search.best_params_)\n",
    "print(\"Random Forest Validation RMSE:\", np.sqrt(mean_squared_error(y_val, rf_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df641c6-9e12-408b-945f-c29aadc6d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 20000\n",
      "max_resources_: 285793\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 14\n",
      "n_resources: 20000\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 5\n",
      "n_resources: 60000\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 180000\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Total elapsed time: 2245.30 seconds\n",
      "Gradient Boosting Best Params: {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "Gradient Boosting Validation RMSE: 3005.6031338390785\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gb_param_grid = {  \n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "gb_search = HalvingRandomSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=gb_param_grid,\n",
    "    factor=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5, \n",
    "    resource='n_samples',\n",
    "    min_resources=20000,\n",
    "    max_resources='auto',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "gb_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "gb_best = gb_search.best_estimator_\n",
    "gb_preds = gb_best.predict(X_val)\n",
    "print(\"Gradient Boosting Best Params:\", gb_search.best_params_)\n",
    "print(\"Gradient Boosting Validation RMSE:\", np.sqrt(mean_squared_error(y_val, gb_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505d300-d8bc-463c-8a62-277ea4f25879",
   "metadata": {},
   "source": [
    "Since `HalvingRandomSearchCV` uses successive elimination, not all parameter combinations are fully tested.\n",
    "The best `n_estimators=400` does not guarantee that `500` would perform worse.\n",
    "I plan to further improve validation performance by testing higher values of the `n_estimators` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72675af4-62bd-4eb3-862c-6d3fe0a0a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=400 -> RMSE: 3005.60\n",
      "n_estimators=450 -> RMSE: 2966.98\n",
      "n_estimators=500 -> RMSE: 2934.58\n"
     ]
    }
   ],
   "source": [
    "gb_best_params = {'n_estimators': 400, 'min_samples_split': 5, \n",
    "                  'min_samples_leaf': 2, 'max_depth': 10, 'learning_rate': 0.1}\n",
    "\n",
    "for n in [400, 450, 500]:\n",
    "    params = gb_best_params.copy()\n",
    "    params['n_estimators'] = n\n",
    "\n",
    "    gb = GradientBoostingRegressor(**params, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    preds = gb.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"n_estimators={n} -> RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf5d93-2d43-4f61-bab3-9dd302dfcbcd",
   "metadata": {},
   "source": [
    "### Final Model Selection  \n",
    "\n",
    "After hyperparameter tuning and additional testing with different values of `n_estimators`,  \n",
    "the **GradientBoostingRegressor** has been selected as the final model.  \n",
    "\n",
    "The chosen best parameters are:  \n",
    "- `n_estimators = 500`  \n",
    "- `min_samples_split = 5`  \n",
    "- `min_samples_leaf = 2`  \n",
    "- `max_depth = 10`  \n",
    "- `learning_rate = 0.1`  \n",
    "\n",
    "This configuration provided the best validation performance and will be used for the final test evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55350604-0bbd-4d1c-b008-3de6a891ab14",
   "metadata": {},
   "source": [
    "### Test Set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d44eb9-d4cb-41c1-91f9-700cbe549dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor RMSE on test set: 2795.82\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'n_estimators': 500,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1\n",
    "}\n",
    "\n",
    "final_model = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "final_preds = final_model.predict(X_test)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "\n",
    "model_name = \"GradientBoostingRegressor\"\n",
    "print(f\"{model_name} RMSE on test set: {final_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbdf97-4a66-48de-be12-3b5450acd394",
   "metadata": {},
   "source": [
    "### Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e62b62e8-243f-4451-9109-0c168eae00a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GradientBoostingRegressor saved to final_model_gb.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, \"final_model_gb.pkl\")\n",
    "print(\"Final GradientBoostingRegressor saved to final_model_gb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19aad82c-5253-4663-bda7-6c710f6c7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model RMSE on test set: 2795.82\n"
     ]
    }
   ],
   "source": [
    "# To verify:\n",
    "loaded_model = joblib.load(\"final_model_gb.pkl\")\n",
    "\n",
    "loaded_preds = loaded_model.predict(X_test)\n",
    "loaded_rmse = np.sqrt(mean_squared_error(y_test, loaded_preds))\n",
    "print(f\"Reloaded model RMSE on test set: {loaded_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561c639-025c-4ee4-b19c-6afe3d6660bc",
   "metadata": {},
   "source": [
    "#### Forecast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "627258f2-6eeb-4b73-98bf-05209e886a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts saved to ../data/processed/forecasts_single_sku.csv\n"
     ]
    }
   ],
   "source": [
    "forecast_df = pd.DataFrame({\n",
    "    \"t\": range(len(final_preds)),   # period index (0,1,2,...)\n",
    "    \"demand\": final_preds           # forecasted sales\n",
    "})\n",
    "\n",
    "# Save to processed folder for optimization step\n",
    "forecast_path = \"../data/processed/forecasts_single_sku.csv\"\n",
    "forecast_df.to_csv(forecast_path, index=False)\n",
    "print(\"Forecasts saved to\", forecast_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
